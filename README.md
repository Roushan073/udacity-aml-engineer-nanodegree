<!-- #region -->
# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**Problem Statement**
We use the bank marketing dataset that contains different demographic and contractual information about bank clients as well as a target variable that determines whether the client subscribed to a term deposit with the bank. We want to use this data to create a classifier that can predict whether other clients will subscribe to a term deposit with the bank.

**Solution**
The best performing model was a XGBoost classifier that was trained as part of an automl experiment. We use accuracy as the evaluation metric and the model reached an accuracy of %.


## Scikit-learn Pipeline
**Pipeline Architecture**
The udacity-project.ipynb contains the code to execute the Scikit-learn pipeline and is made up of three different parts. In the first part, setup code is executed which creates a workspace and experiment object and starts an initial run of the experiment. In the second part, a hyperdrive run is configured for hyperparameter tuning with a logistic regression model. This includes a parameter sampler as well as an early stopping policy, an SKLearn estimator, a training entry script and a hyperdrive run configuration. This run configuration is used to submit an experiment to an AML compute cluster which will then conduct the hyperparameter tuning based on the given input. The training entry script contains code for the data preparation, the data splitting and the actual model training. In order to access the data, the TabularDatasetFactory class is used to point to an URI. The logistic regression model that is trained can be tuned on the regularization parameter C and the max iterations parameter. The argparse library is used in the entry script to enable passing these hyperparameters as input to the script. The hyperparameters as well as accuracy as the evaluation metric are logged in the experiment run. In the third part of the pipeline, an automl run is configured using the same dataset and data cleaning code as the hyperdrive run. This is done with the AutoMLConfig class.  

**Benefits of the Parameter Sampler**
The parameter sampler allows to define a parameter space that is passed to the training entry script and on which the hyperparameter tuning is conducted. The AzureML SDK contains useful classes to make the definition of the parameter space simple. Specifically, we use the RandomParameterSampling class with a loguniform search space of the hyperparameter "C" and a choice search space of the hyperparameter "max_iter". Random parameter sampling is more efficient than grid search parameter sampling and achieves almost similar accuracy.

**Benefits of the Early Stopping Policy**
The AzureML SDK also allows to pass an early stopping policy to the hyperdrive run configuration. Specifically, we use the BanditPolicy class. This enables to stop not very promising hyperparameter sample runs early to save on compute costs and reduce the time to arrive at a well-performing model.


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
**Model and Hyperparameters generated by AutoML**
As part of the automl run many different models have been generated and tuned automatically. This includes for example 

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
**Comparison of the two Models' Accuracy and Architecture**
Accuracy:
The best logistic regression model of the hyperdrive run achieved an accuracy of 90.72%. This is a quite good accuracy given that the logistic regression model is a very simple model.

Architecture:
The hyperdrive run requires a custom implementation of a training entry script that contains the code for model training whereas the automl run trains models automatically. For the hyperdrive run we split the data into training and validation set with a ratio of 80:20. For the automl run a 5-fold cross validation is used. The automl run also requires a single dataset passed to it, meaning that the dataset should contain features and target. The data cleaning function includes a split input parameter to accomodate this requirement. Moreover, it is necessary to implement certain parts of the automl run in the pipeline notebook itself as there is no training entry script. This includes the creation of the dataset and the data cleaning. Last but not least, there are also some differences in the run API depending on whether the run is a hyperdrive or automl run. For the hyperdrive run the model is registered directly from the run object while the model was downloaded to the Compute Instance and registered from there for the automl run.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
**Areas of Improvement for Future Experiments**
Feature engineering can be used to create more powerful features than just the default columns provided through the dataset. A custom XGBoost
<!-- #endregion -->
