<!-- #region -->
# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**Problem Statement**

We use the bank marketing dataset that contains different demographic and client information about bank clients as well as a target variable that determines whether the client subscribed to a term deposit with the bank. A classifier shall be trained on this data to predict whether other clients will subscribe to a term deposit with the bank.

**Solution**

Azure Machine Learning Hyperdrive and AutoML are used to train different classifiers. The best performing model is a XGBoost classifier that is trained as part of the automl experiment. We use accuracy as the evaluation metric and the model reaches an accuracy of 91.52%.


## Scikit-learn Pipeline
**Pipeline Architecture**

The "udacity-project.ipynb" notebook contains the code to execute the Scikit-learn "pipeline" and is made up of three different parts. In the first part, setup code is executed which creates a workspace and experiment object and starts an initial run of the experiment. In the second part, a hyperdrive run is configured for hyperparameter tuning with a logistic regression model. This includes a parameter sampler as well as an early stopping policy, a SKLearn estimator, a training entry script and a hyperdrive run configuration. This run configuration is used to submit an experiment to an AML compute cluster which will then conduct the hyperparameter tuning based on the given input. The training entry script contains code for the data preparation, the data splitting and the actual model training. In order to access the data, the TabularDatasetFactory class is used to point to an URI. The logistic regression model that is trained can be tuned on the regularization parameter C and the max iterations parameter. The argparse library is used in the entry script to enable passing these hyperparameters as input to the script. The hyperparameters as well as accuracy as the evaluation metric are logged in the experiment run. In the third part of the pipeline, an automl run is configured using the same dataset and data cleaning code as the hyperdrive run. This is done with the AutoMLConfig class.  

**Benefits of the Parameter Sampler**

The parameter sampler allows to define a parameter space that is passed to the training entry script and on which the hyperparameter tuning is conducted. The AzureML SDK contains useful classes to make the definition of the parameter space simple. Specifically, we use the RandomParameterSampling class with a loguniform search space of the hyperparameter "C" and a choice search space of the hyperparameter "max_iter". Random parameter sampling is more efficient than grid search parameter sampling and achieves almost similar accuracy. A loguniform search space is used to ensure that small values for C are represented in an adequate amount among the random samples.

**Benefits of the Early Stopping Policy**

The AzureML SDK also allows to pass an early stopping policy to the hyperdrive run configuration. Specifically, we use the BanditPolicy class. This enables to stop not very promising hyperparameter sample runs early to save on compute costs and reduce the time to arrive at a well-performing model. The Bandit policy is empirically among the best early stopping policies.


## AutoML
**Model and Hyperparameters generated by AutoML**

As part of the automl run many different models have been generated and tuned automatically. This includes for example tree-based models such as a XGBoostClassifier and a LightGBM as well as ensemble models such as a VotingEnsemble and StackEnsemble. The best performing model of the automl run was a XGBoostClassifer with an accuracy of 91.52%. This classifier uses a MaxAbsScaler for scaling and has the following hyperparameters (among others):
- gamma=0
- learning_rate=0.1
- max_depth=3
- min_child_weight=1
- missing=None
- n_estimators=100
- reg_alpha=0
- reg_lambda=1

XGBoost classifiers are traditionally among the best for classification tasks.

## Pipeline comparison
**Comparison of the two Models' Accuracy and Architecture**

Accuracy:
The best logistic regression model of the hyperdrive run achieves an accuracy of 90.72%. This is a quite good accuracy given that the logistic regression model is a very simple model.
The best model of the automl run achieves an accuracy of 91.52%. This better performance is mostly due to the fact that we allow more complex models and the automl run basically has a more flexible choice of algorithms.

Architecture:
The hyperdrive run requires a custom implementation of a training entry script that contains the code for model training whereas the automl run trains models automatically. For the hyperdrive run we split the data into training and validation set with a ratio of 80:20. For the automl run a 5-fold cross validation is used. The automl run also requires a single dataset passed to it, meaning that the dataset should contain features and target. The data cleaning function includes a split input parameter to accomodate for this requirement. Moreover, it is necessary to implement certain parts of the automl run in the pipeline notebook itself as there is no training entry script. This includes the creation of the dataset and the data cleaning. Last but not least, there are also some differences in the run API depending on whether the run is a hyperdrive or automl run. For the hyperdrive run the model is registered directly from the run object while the model is downloaded to the Compute Instance and registered from there for the automl run.

## Future work
**Areas of Improvement for Future Experiments**

Feature engineering can be used to create more powerful features than just the default columns provided by the dataset.
A custom XGBoost classifier can be trained using hyperdrive to optimize the hyperparameters. Cross-validation should be used for training this classifier to not lose any data for training.
<!-- #endregion -->

```python

```
